{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TCFile import TCFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\rkka_Projects\\cell_death_v1\\Data\\Hela_Segmentation_test_PWS\\TCF\\230510.174730.HeLa_Hoechst.001.Group1.A2.T001P25.TCF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio  # pip install imageio\n",
    "from TCFile import TCFile\n",
    "\n",
    "def preprocess_and_save_to_png(\n",
    "    tcf_paths,\n",
    "    output_dir,\n",
    "    time_indices_live=(0, 1),\n",
    "    time_indices_dead=(35, 36),\n",
    "    mode='qpi',\n",
    "    crop_size=(480, 480),  # change to the size you want,\n",
    "    patch_num = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess 3D tomography data from TCF files, create a 2D projection,\n",
    "    crop into tiles (no overlap), and save each tile as an 8-bit PNG.\n",
    "\n",
    "    Args:\n",
    "        tcf_paths (list of str): Paths to TCF files.\n",
    "        output_dir (str): Directory to save output subfolders ('live', 'dead').\n",
    "        time_indices_live (tuple): Time indices for 'live' label.\n",
    "        time_indices_dead (tuple): Time indices for 'dead' label.\n",
    "        mode (str): 'qpi' (sum across z) or 'mip' (max intensity projection).\n",
    "        crop_size (tuple): (crop_height, crop_width) for each tile.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output subdirectories for each label\n",
    "    live_dir = os.path.join(output_dir, \"live\")\n",
    "    dead_dir = os.path.join(output_dir, \"dead\")\n",
    "    os.makedirs(live_dir, exist_ok=True)\n",
    "    os.makedirs(dead_dir, exist_ok=True)\n",
    "\n",
    "    for path in tcf_paths:\n",
    "        print(f\"Processing file: {path}\")\n",
    "\n",
    "        # Open the TCF file in '3D' mode\n",
    "        tcf_file = TCFile(path, '3D')\n",
    "\n",
    "        # Process the 'live' time indices\n",
    "        for t_idx in time_indices_live:\n",
    "            # data_3d has shape (z, y, x)\n",
    "            data_3d = tcf_file[t_idx]\n",
    "\n",
    "            # 2D projection (sum or max)\n",
    "            if mode.lower() == 'qpi':\n",
    "                slice_2d = np.sum(data_3d, axis=0)  # shape: (y, x)\n",
    "            elif mode.lower() == 'mip':\n",
    "                slice_2d = np.max(data_3d, axis=0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "            # Crop the 2D slice\n",
    "            _crop_and_save(slice_2d, live_dir, path, t_idx, mode, crop_size, patch_num)\n",
    "\n",
    "        # Process the 'dead' time indices\n",
    "        for t_idx in time_indices_dead:\n",
    "            data_3d = tcf_file[t_idx]\n",
    "            \n",
    "            if mode.lower() == 'qpi':\n",
    "                slice_2d = np.sum(data_3d, axis=0)\n",
    "            elif mode.lower() == 'mip':\n",
    "                slice_2d = np.max(data_3d, axis=0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "            _crop_and_save(slice_2d, dead_dir, path, t_idx, mode, crop_size, patch_num)\n",
    "\n",
    "\n",
    "def _crop_and_save(slice_2d, out_dir, file_path, time_idx, mode, crop_size, patch_num):\n",
    "    \"\"\"\n",
    "    Given a 2D array (slice_2d), split it into non-overlapping\n",
    "    tiles (of size crop_size) and save each tile as PNG.\n",
    "    \"\"\"\n",
    "\n",
    "    crop_h, crop_w = crop_size\n",
    "    height, width = slice_2d.shape\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Number of vertical/horizontal steps (discard remainder if it doesn't fit exactly)\n",
    "    num_steps_y = height // crop_h\n",
    "    num_steps_x = width // crop_w\n",
    "\n",
    "    # For each tile\n",
    "    patch_step = 0\n",
    "    for iy in range(num_steps_y):\n",
    "        for ix in range(num_steps_x):\n",
    "            top = iy * crop_h\n",
    "            left = ix * crop_w\n",
    "            patch = slice_2d[top:top+crop_h, left:left+crop_w]\n",
    "\n",
    "            # Min-max normalization (float64 -> uint8)\n",
    "            min_val, max_val = patch.min(), patch.max()\n",
    "            if max_val - min_val < 1e-10:\n",
    "                norm_patch = np.zeros_like(patch, dtype=np.uint8)\n",
    "            else:\n",
    "                norm_patch = ((patch - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "\n",
    "            # Construct output filename\n",
    "            # Example: \"file_t{0}_y0_x0_mode-mip.png\"\n",
    "            \"\"\"\n",
    "            out_name = (\n",
    "                f\"{base_name}_t{time_idx}\"\n",
    "                f\"_y{iy}_x{ix}\"\n",
    "                f\"_mode-{mode}.png\"\n",
    "            )\n",
    "            \"\"\"\n",
    "            out_name = (\n",
    "                str(time_idx).zfill(2) + \".png\"\n",
    "            )\n",
    "            out_path = os.path.join(out_dir, out_name)\n",
    "\n",
    "            # Save the tile\n",
    "            imageio.imwrite(out_path, norm_patch)\n",
    "            \n",
    "            patch_step += 1\n",
    "\n",
    "            if patch_step == patch_num :\n",
    "                break\n",
    "        if patch_step == patch_num :\n",
    "                break\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage:\n",
    "# -----------------------------\n",
    "# Example TCF paths\n",
    "tcf_paths = [\n",
    "    r\"C:\\rkka_Projects\\cell_death_v1\\Data\\Hela_Segmentation_test_PWS\\TCF\\230510.174730.HeLa_Hoechst.001.Group1.A2.T001P25.TCF\"\n",
    "]\n",
    "output_directory = r\"C:\\rkka_Projects\\cell_death_v1\\Data\\test\"\n",
    "\n",
    "preprocess_and_save_to_png(\n",
    "    tcf_paths,\n",
    "    output_dir=output_directory,\n",
    "    time_indices_live=(i for i in range(18)),\n",
    "    time_indices_dead=(i+18 for i in range(18)),\n",
    "    mode='mip',        # or 'mip'\n",
    "    crop_size=(480, 480),  # adjust as needed\n",
    "    patch_num= 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Augmentation Transform\n",
    "augmentation_transform_2d = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=90),\n",
    "])\n",
    "\n",
    "class ImageDatasetWithAugmentation(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augmentation=True, rescale=True, gaussian=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the root directory containing 'live' and 'dead' subfolders.\n",
    "            transform (callable, optional): Additional preprocessing or transforms.\n",
    "            augmentation (bool): Whether to apply data augmentation.\n",
    "            rescale (bool): Whether to rescale image intensities to [0, 1].\n",
    "            gaussian (bool): Whether to apply Gaussian blur.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.rescale = rescale\n",
    "        self.gaussian = gaussian\n",
    "\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect file paths and labels\n",
    "        for label, class_name in enumerate(['dead', 'live']):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith('.png'):\n",
    "                    self.samples.append(os.path.join(class_dir, filename))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Load as grayscale\n",
    "\n",
    "        # Convert to numpy array\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Rescale\n",
    "        if self.rescale:\n",
    "            image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "\n",
    "        # Convert to tensor and expand to 3 channels\n",
    "        image_tensor = torch.from_numpy(image_np).float().unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "        # Gaussian blur\n",
    "        if self.gaussian:\n",
    "            image_tensor = T.GaussianBlur(kernel_size=(5, 5))(image_tensor)\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.augmentation:\n",
    "            edge_mean = calculate_edge_mean_2d(image_tensor)\n",
    "            image_tensor = augmentation_transform_2d(image_tensor)\n",
    "            image_tensor[image_tensor == 0] = edge_mean\n",
    "\n",
    "        # Additional transforms\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        return image_tensor, label\n",
    "\n",
    "\n",
    "def calculate_edge_mean_2d(tensor):\n",
    "    \"\"\"Calculate mean of edges for a tensor (used in augmentation).\"\"\"\n",
    "    return torch.mean(\n",
    "        torch.cat([tensor[:, 0, :], tensor[:, -1, :], tensor[:, :, 0], tensor[:, :, -1]])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models import resnet101\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from torchvision.transforms import Normalize\n",
    "from torch_tomogram_dataset import AugmentedDatasetWrapper\n",
    "\n",
    "# Dataset Parameters\n",
    "train_data_dir = r\"C:\\rkka_Projects\\cell_death_v1\\Data\\output\\train\"\n",
    "val_data_dir = r\"C:\\rkka_Projects\\cell_death_v1\\Data\\output\\val\"\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = models.ResNet101_Weights.IMAGENET1K_V2.transforms()\n",
    "train_dataset = ImageDatasetWithAugmentation(root_dir=train_data_dir, transform=transform, augmentation=True)\n",
    "val_dataset = ImageDatasetWithAugmentation(root_dir=val_data_dir, transform=transform, augmentation=False)\n",
    "\n",
    "# Augment wrapping\n",
    "train_dataset = AugmentedDatasetWrapper(train_dataset, num_repeats=3)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNetV2\n",
    "model = resnet101(pretrained=True)  # Use EfficientNetV2-Small\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(num_features, 2)  # Binary classification (live/dead)\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer4.2' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = resnet101(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(num_features, 2)\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer4.2' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "model = model.cuda()\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate loss and back propagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 42,504,258\n",
      "Trainable Parameters: 4,466,690\n"
     ]
    }
   ],
   "source": [
    "# Count total and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:09<02:55,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.0012, Train Acc: 0.9870, Val Loss: 0.0005, Val Acc: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:18<02:44,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 0.0004, Train Acc: 0.9980, Val Loss: 0.0009, Val Acc: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:27<02:34,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 0.0005, Train Acc: 0.9950, Val Loss: 0.0010, Val Acc: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:36<02:26,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 0.0003, Train Acc: 0.9990, Val Loss: 0.0010, Val Acc: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:39<02:36,  9.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m train_loss, train_correct, train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\miniconda3\\envs\\cell\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\rkka_Projects\\cell_death_v1\\torch_tomogram_dataset\\augmented_dataset_wrapper.py:15\u001b[0m, in \u001b[0;36mAugmentedDatasetWrapper.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Map the index to a sample in the original dataset\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     original_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m---> 15\u001b[0m     sample, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[original_idx]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Each time we fetch the sample, it will have an augmentation applied\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample, label\n",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m, in \u001b[0;36mImageDatasetWithAugmentation.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     55\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m (image_np \u001b[38;5;241m-\u001b[39m image_np\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (image_np\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m image_np\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Convert to tensor and expand to 3 channels\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(image_np)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Gaussian blur\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaussian:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training Loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += len(labels)\n",
    "\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += len(labels)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "        f\"Train Loss: {train_loss/train_total:.4f}, Train Acc: {train_correct/train_total:.4f}, \"\n",
    "        f\"Val Loss: {val_loss/val_total:.4f}, Val Acc: {val_correct/val_total:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c, preds = torch.max(outputs, 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\김민욱\\AppData\\Local\\Temp\\ipykernel_1664\\3266578923.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(outputs)[2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0317, 0.9683], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(outputs)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqpi_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mstate_dict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.save('qpi_model.pth', model.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 00.png: Death Probability = 0.0003\n",
      "Processed 01.png: Death Probability = 0.0005\n",
      "Processed 02.png: Death Probability = 0.0002\n",
      "Processed 03.png: Death Probability = 0.0070\n",
      "Processed 04.png: Death Probability = 0.0007\n",
      "Processed 05.png: Death Probability = 0.0257\n",
      "Processed 06.png: Death Probability = 0.0005\n",
      "Processed 07.png: Death Probability = 0.0007\n",
      "Processed 08.png: Death Probability = 0.0012\n",
      "Processed 09.png: Death Probability = 0.0032\n",
      "Processed 10.png: Death Probability = 0.0115\n",
      "Processed 11.png: Death Probability = 0.0175\n",
      "Processed 12.png: Death Probability = 0.0108\n",
      "Processed 13.png: Death Probability = 0.0028\n",
      "Processed 14.png: Death Probability = 0.0904\n",
      "Processed 15.png: Death Probability = 0.0457\n",
      "Processed 16.png: Death Probability = 0.9558\n",
      "Processed 17.png: Death Probability = 0.5270\n",
      "Processed 18.png: Death Probability = 0.9790\n",
      "Processed 19.png: Death Probability = 0.9840\n",
      "Processed 20.png: Death Probability = 0.9969\n",
      "Processed 21.png: Death Probability = 0.9706\n",
      "Processed 22.png: Death Probability = 0.9854\n",
      "Processed 23.png: Death Probability = 0.9802\n",
      "Processed 24.png: Death Probability = 0.9961\n",
      "Processed 25.png: Death Probability = 0.9979\n",
      "Processed 26.png: Death Probability = 0.9996\n",
      "Processed 27.png: Death Probability = 0.9985\n",
      "Processed 28.png: Death Probability = 0.9925\n",
      "Processed 29.png: Death Probability = 0.9996\n",
      "Processed 30.png: Death Probability = 0.9987\n",
      "Processed 31.png: Death Probability = 0.9973\n",
      "Processed 32.png: Death Probability = 0.9760\n",
      "Processed 33.png: Death Probability = 0.9959\n",
      "Processed 34.png: Death Probability = 0.9656\n",
      "Processed 35.png: Death Probability = 0.9837\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "model = model.cuda()\n",
    "\n",
    "# Transform for input preprocessing\n",
    "transform = models.ResNet101_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "# Folder containing timelapse PNG images\n",
    "test_folder = r\"C:\\rkka_Projects\\cell_death_v1\\Data\\test\"\n",
    "\n",
    "# Ensure the files are sorted by their names\n",
    "file_list = sorted([f for f in os.listdir(test_folder) if f.endswith('.png')])\n",
    "\n",
    "# List to store death probabilities\n",
    "death_probabilities = []\n",
    "\n",
    "# Predict death probabilities for sorted PNG files\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(test_folder, filename)\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(file_path).convert('RGB')  # Ensure RGB channels\n",
    "        input_tensor = transform(image).unsqueeze(0).cuda()  # Add batch dimension\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(input_tensor)  # Shape: (1, 2)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)  # Convert to probabilities\n",
    "\n",
    "        # Extract the death probability (class index 1)\n",
    "        death_probability = probabilities[0, 0].item()\n",
    "        death_probabilities.append(death_probability)\n",
    "\n",
    "        print(f\"Processed {filename}: Death Probability = {death_probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c7390baea0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ylim(0,1)\n",
    "plt.plot(death_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_list = []\n",
    "\n",
    "for f in os.listdir(r\"C:\\rkka_Projects\\cell_death_v1\\Data\\test\"):\n",
    "    if f.endswith('.png'):\n",
    "        file_list.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_tomogram_dataset.utils import calculate_edge_mean_2d\n",
    "from torch_tomogram_dataset.transforms import augmentation_transform_2d\n",
    "from PIL import Image\n",
    "\n",
    "class Cata(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augmentation=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.augmentation = augmentation\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        \n",
    "        states = ['live', 'dead']\n",
    "        for label, state in enumerate(states):\n",
    "            path = os.path.join(self.root_dir, state)\n",
    "            for file in os.listdir(path):\n",
    "                if file.endswith('.png'):\n",
    "                    self.samples.append(os.path.join(path, file))\n",
    "                    self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.samples[idx]).convert('L')\n",
    "        image = np.array(image)\n",
    "        image = torch.from_numpy(image)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if transform is not None:\n",
    "            image = transform(image)\n",
    "        \n",
    "        # Rotation with edge mean filling\n",
    "        if self.augmentation:\n",
    "            edge_mean = calculate_edge_mean_2d(image_tensor)\n",
    "            image_tensor = augmentation_transform_2d(image_tensor)\n",
    "            image_tensor[image_tensor==0] = edge_mean\n",
    "        \n",
    "        return image, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
